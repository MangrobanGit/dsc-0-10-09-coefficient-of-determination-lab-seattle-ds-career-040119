{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficient of Determination - Lab\n",
    "\n",
    "## Introduction\n",
    "In the previous lesson we looked at the coefficient of determination, what it means and how it is calculated. In this lesson, we shall use the R-squared formula to calculate it in python and numpy. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Mathematically calculate R squared using a toy dataset\n",
    "\n",
    "* Calculate the coefficient of determination (R-squared) for a given regression line\n",
    "\n",
    "* Interpret the value of R-squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "\n",
    "Once a regression model is created, we need to decide how \"accurate\" the regression line is to some degree. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the the equation for R-squared again: \n",
    "\n",
    "![](rs1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# potential new image\n",
    "![](r2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation is essentially 1 minus the division of the **squared error of the regression (predicted) line**, by the **squared error of the mean y line**\n",
    ". \n",
    ">The mean y line is quite literally the mean of all of the y values from the dataset. Thus, we do the squared error of the average y, and of the regression line. \n",
    "\n",
    "The objective here is to learn how much of the error is actually just simply a result in variation in the data features, as opposed to being a result of the regression line being a poor fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming R-squared\n",
    "\n",
    "Let's calculate R-squared in Python. The first step would be to calculate the squared error. Remember squared error is the sum of quares of difference between a given line and the ground truth (actual data points).\n",
    "\n",
    "Create a function `sq_err()` that takes in y points for two lines as arrays, calculates the difference corresponding elements of these arrays, squares, and sums all the differences. The function should return the SSE value as we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sum of squared errors between regression and mean line \n",
    "import numpy as np\n",
    "\n",
    "def sq_err(ys_a, ys_b):\n",
    "    \"\"\"\n",
    "    input\n",
    "    ys_a : regression line\n",
    "    ys_b : mean line\n",
    "    \n",
    "    return\n",
    "    squared error between regression and true line (ss_tot)\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum(np.square(np.subtract(ys_a, ys_b)))\n",
    "    \n",
    "# Check the output with some toy data\n",
    "Y_a = np.array([1,3,5,7])\n",
    "Y_b = np.array([1,4,5,8])\n",
    "\n",
    "sq_err(Y_a, Y_b)\n",
    "\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squared error, as calculated above is only a part of the coefficient of determination, Let's now build a function that would use `sq_err()` function above to calculate the value of r-squared by first calculating SSE and SST (SSres and SStot above) and then plugging these values into the R-squared formula. Perform following tasks\n",
    "* Calculate the mean of ys_real\n",
    "* Calculate SSE using `sq_err()`\n",
    "* Calculate SST using `sq_err()`\n",
    "* Calculate R-squared from above values using given formula. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(1)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Y_mean , squared error for regression and mean line , and calculate r-squared\n",
    "\n",
    "def r_squared(ys_real, ys_predicted):\n",
    "    \"\"\"\n",
    "    input\n",
    "    ys_real: real values\n",
    "    ys_predicted: regression values\n",
    "    \n",
    "    return\n",
    "    r_squared value\n",
    "    \"\"\"\n",
    "    # SSE \n",
    "    sse = sq_err(ys_real, ys_predicted)\n",
    "    \n",
    "    # SSR\n",
    "    ssr = sq_err(Y_b, np.mean(Y_a))\n",
    "    \n",
    "    # SST\n",
    "    y_hat_line = np.ones(len(Y_a)) * np.mean(Y_a)\n",
    "    print()\n",
    "    sst = sq_err(ys_real, y_hat_line)\n",
    "    sst = sq_err(ys_real, np.mean(ys_real))\n",
    "    \n",
    "    #return 1 - (sse/sst)\n",
    "    return 1-(sse/sst)\n",
    "\n",
    "# Check the output with some toy data\n",
    "Y_real = np.array([1,3,5,7])\n",
    "Y_pred = np.array([1,5,5,10])\n",
    "\n",
    "r_squared(Y_real, Y_pred)\n",
    "\n",
    "# 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8da81e54c5a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Check the output with some toy data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mY_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mY_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def sq_err(ys_a, ys_b):\n",
    "    \"\"\"\n",
    "    input\n",
    "    ys_a : regression line\n",
    "    ys_b : mean line\n",
    "    \n",
    "    return\n",
    "    squared error between regression and true line (ss_tot)\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum(np.square(np.subtract(ys_a, ys_b)))\n",
    "    \n",
    "# Check the output with some toy data\n",
    "Y_a = np.array([1,3,5,7])\n",
    "Y_b = np.array([1,4,5,8])\n",
    "\n",
    "sq_err(Y_a, Y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/statsmodels/stats/stattools.py:72: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n",
      "  \"samples were given.\" % int(n), ValueWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>Y_b</td>       <th>  R-squared:         </th> <td>   0.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   60.50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 11 Apr 2019</td> <th>  Prob (F-statistic):</th>  <td>0.0161</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:20:21</td>     <th>  Log-Likelihood:    </th> <td> -2.4569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     4</td>      <th>  AIC:               </th> <td>   8.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     2</td>      <th>  BIC:               </th> <td>   7.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.1000</td> <td>    0.648</td> <td>    0.154</td> <td> 0.892</td> <td>   -2.688</td> <td>    2.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_a</th>       <td>    1.1000</td> <td>    0.141</td> <td>    7.778</td> <td> 0.016</td> <td>    0.492</td> <td>    1.708</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   3.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.000</td> <th>  Prob(JB):          </th> <td>   0.857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.640</td> <th>  Cond. No.          </th> <td>    9.74</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    Y_b   R-squared:                       0.968\n",
       "Model:                            OLS   Adj. R-squared:                  0.952\n",
       "Method:                 Least Squares   F-statistic:                     60.50\n",
       "Date:                Thu, 11 Apr 2019   Prob (F-statistic):             0.0161\n",
       "Time:                        22:20:21   Log-Likelihood:                -2.4569\n",
       "No. Observations:                   4   AIC:                             8.914\n",
       "Df Residuals:                       2   BIC:                             7.686\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.1000      0.648      0.154      0.892      -2.688       2.888\n",
       "Y_a            1.1000      0.141      7.778      0.016       0.492       1.708\n",
       "==============================================================================\n",
       "Omnibus:                          nan   Durbin-Watson:                   3.400\n",
       "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.308\n",
       "Skew:                           0.000   Prob(JB):                        0.857\n",
       "Kurtosis:                       1.640   Cond. No.                         9.74\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'Y_b~Y_a'\n",
    "\n",
    "model = ols(formula = f, data = dats).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very low value , but it was not from some real data. So now we have quite a few functions for calculating slope, intercept, bestfit line, plotting and calculating R-squared. In the next lab we'll put these all together to run a complete regression experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lesson we saw how to calculate the R-squared value in python and numpy. Following lab will require you put all the functions from last few labs together to create a complete DIY regression experiment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
